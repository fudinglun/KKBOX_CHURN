{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c882989f1d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import re\n",
    "import xgboost ; import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "song = pd.read_csv('songs.csv')\n",
    "members =  pd.read_csv('members.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(song,on= 'song_id',how = 'left')\n",
    "test = test.merge(song,on = 'song_id',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user_bias, user_freq, song_bias, song_freq\n",
    "tuf = train.groupby('msno')['target'].agg(['sum','count']) #Laplacian smoothing\n",
    "tuf = tuf.reset_index()\n",
    "tuf.columns.values[1:3] = ['user_sum','user_count']\n",
    "train = train.merge(tuf,on = 'msno',how = 'left')\n",
    "train['user_freq'] = (train.user_sum-train.target+1)/(train.user_count+1)\n",
    "train['user_count'] = train['user_count'] - 1\n",
    "train = train.drop('user_sum',1)\n",
    "\n",
    "#prepare test's user bias\n",
    "tuf['user_freq'] = (tuf.user_sum+1)/(tuf.user_count+2)\n",
    "test = test.merge(tuf,on = 'msno',how = 'left')\n",
    "test.loc[pd.isnull(test.user_count),'user_count'] = 0\n",
    "test.loc[pd.isnull(test.user_freq),'user_freq'] = 0.5\n",
    "test = test.drop('user_sum',1)\n",
    "\n",
    "#prepare songs' bias\n",
    "tuf = train.groupby('song_id')['target'].agg(['sum','count']) #Laplacian smoothing\n",
    "tuf = tuf.reset_index()\n",
    "tuf.columns.values[1:3] = ['song_sum','song_count']\n",
    "train = train.merge(tuf,on = 'song_id',how = 'left')\n",
    "train['song_freq'] = (train.song_sum-train.target+1)/(train.song_count+1)\n",
    "train['song_count'] = train['song_count'] -1\n",
    "train = train.drop('song_sum',1)\n",
    "\n",
    "#prepare test's songs' bias\n",
    "tuf['song_freq'] = (tuf.song_sum+1)/(tuf.song_count+2)\n",
    "test = test.merge(tuf,on = 'song_id',how = 'left')\n",
    "test.loc[pd.isnull(test.song_count),'song_count'] = 0\n",
    "test.loc[pd.isnull(test.song_freq),'song_freq'] = 0.5\n",
    "test = test.drop('song_sum',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artist Preparation: \n",
    "1. find the intersection of artist between the train and test\n",
    "2. treat the compound artist seperately from single artist\n",
    "3. if a single artist from test set hasn't appeared before, we can do nothing, however, if a compound artist has appeared, we can possibly estimate it from the single artist's contribution\n",
    "4. add a dummy variable artist_estimated to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    l = re.split(\"[+|&/]\",x)\n",
    "    l = [i.strip() for i in list(l)]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.artist_name = train.artist_name.fillna(\"no_rec\")\n",
    "test.artist_name = test.artist_name.fillna(\"no_rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_art = set()\n",
    "for name in train.artist_name.unique():\n",
    "    train_art.add(name)\n",
    "    for sub in split(name):\n",
    "        train_art.add(sub.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_art_diff = set()\n",
    "for name in test.artist_name.unique():\n",
    "    indic = [sub.strip() in train_art for sub in split(name)]\n",
    "    if not any(indic):\n",
    "        test_art_diff.add(name)\n",
    "#19%的artist从来没有出现过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_csame = set()\n",
    "for name in test.artist_name.unique():\n",
    "     if len(split(name)) > 1 and name in train_art:\n",
    "            tt_csame.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_single = set()\n",
    "for name in train.artist_name.unique():\n",
    "    if len(split(name)) == 1:\n",
    "        tt_single.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_cdump = set()\n",
    "for name in train.artist_name.unique():\n",
    "    if len(split(name)) > 1 and not name in tt_csame:\n",
    "        tt_cdump.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artdict = dict(zip(list(train_art),range(len(train_art))))\n",
    "userdict = dict(zip(train.msno.unique(),range(train.msno.nunique())))\n",
    "v = train.groupby(['msno','artist_name'])['target'].agg(['sum','count'])\n",
    "v = v.reset_index()\n",
    "v.columns.values[2:] = ['uasum','uacount']\n",
    "uamat = sparse.lil_matrix((len(userdict),2*len(artdict)))\n",
    "v['mind'] = v.msno.apply(lambda x: userdict[x])\n",
    "v['amind'] = v.artist_name.apply(lambda x: artdict[x])\n",
    "for i in v.index:\n",
    "    uamat[v.mind[i],2*v.amind[i]] = v.uasum[i]\n",
    "    uamat[v.mind[i],2*v.amind[i]+1] = v.uacount[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disdict = {}\n",
    "for key, ind in artdict.items():\n",
    "    l = [artdict[sub] for sub in split(key) if not sub in tt_single]\n",
    "    if len(l) > 0:\n",
    "        disdict[key] = (l,ind)\n",
    "uamat = sparse.csc_matrix(uamat)\n",
    "for key, ind in disdict.items():\n",
    "    for sub in ind[0]:\n",
    "        uamat[:,2*sub] += uamat[:,2*ind[1]]\n",
    "        uamat[:,2*sub+1] += uamat[:,2*ind[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare train artist feature\n",
    "oart = train.groupby('artist_name')['target'].agg(['sum','count'])\n",
    "oart = oart.reset_index()\n",
    "oart.columns.values[1:3] = ['art_sum','art_count']\n",
    "train = train.merge(oart,on='artist_name',how = 'left')\n",
    "train['art_freq'] = (train.art_sum-train.target+1)/(train.art_count+1)\n",
    "traindump = train[train.artist_name.isin(list(tt_cdump))]\n",
    "train['is_art_dump'] = 0\n",
    "uasum = uamat.sum(axis = 0)\n",
    "for name,group in traindump.groupby(\"artist_name\"):\n",
    "    subind = np.array([artdict[sub.strip()] for sub in split(name)])\n",
    "    gasum = uasum[0,2*subind].sum()/len(subind)\n",
    "    gacount = uasum[0,2*subind+1].sum()/len(subind)\n",
    "    for ind in group.index:\n",
    "        train.set_value(ind,'art_sum',gasum)\n",
    "        train.set_value(ind,'art_count',gacount)\n",
    "        train.set_value(ind,'art_freq',gasum/gacount)\n",
    "        train.set_value(ind,'is_art_dump',1)\n",
    "train = train.drop('art_sum',1)\n",
    "train.art_count = train.art_count -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#prepare test artist feature\n",
    "oart['art_freq'] = oart.art_sum/oart.art_count\n",
    "test = test.merge(oart,on='artist_name',how = 'left')\n",
    "test['is_art_dump'] = 0\n",
    "testnan = test[pd.isnull(test.art_sum)]\n",
    "for name, group in testnan.groupby(\"artist_name\"):\n",
    "    subind = np.array([artdict[sub.strip()] for sub in split(name) if sub.strip() in train_art])\n",
    "    if subind.size == 0:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'art_count',0)\n",
    "            test.set_value(ind,'art_freq',0.5)\n",
    "    else:\n",
    "        gasum = uasum[0,2*subind].sum()/len(subind)\n",
    "        gacount = uasum[0,2*subind+1].sum()/len(subind)\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'art_count',gacount)\n",
    "            test.set_value(ind,'art_freq',gasum/gacount)\n",
    "            test.set_value(ind,'is_art_dump',1)\n",
    "test = test.drop('art_sum',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare train/artist feature\n",
    "train = train.merge(v,on = ['msno','artist_name'],how = 'left')\n",
    "train['uafreq'] = (train['uasum'] - train['target']+1)/(train['uacount']+1)\n",
    "train['uacount'] = train['uacount'] -1\n",
    "for name,group in train[train.artist_name.isin(list(tt_cdump))].groupby('artist_name'):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name)])\n",
    "    vsum = uamat[:,2*subcolin][group.mind,:].sum(axis =1)\n",
    "    vcount = uamat[:,2*subcolin+1][group.mind,:].sum(axis = 1)\n",
    "    gi = group.index ; gs = group.shape[0]\n",
    "    for i in range(gs):\n",
    "        train.set_value(gi[i],'uafreq',vsum[i]/vcount[i] if vcount[i] > 0 else 0.5)\n",
    "        train.set_value(gi[i],'uacount',vcount[i]/len(subcolin))\n",
    "train = train.drop(['mind','amind','uasum'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_genre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e624f3cb0bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtestnu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestnan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtestnan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainmsno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestnu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msno'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'artist_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msubcolin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0martdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_genre\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mindic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moverallfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muasum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0muasum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-e624f3cb0bd1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtestnu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestnan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtestnan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainmsno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestnu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msno'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'artist_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msubcolin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0martdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_genre\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mindic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moverallfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muasum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0muasum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubcolin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_genre' is not defined"
     ]
    }
   ],
   "source": [
    "#prepare test/artist feature\n",
    "trainmsno = train.msno.unique()\n",
    "v['uafreq'] = v.uasum/v.uacount\n",
    "test = test.merge(v,on = ['msno','artist_name'],how = 'left')\n",
    "testnan = test[pd.isnull(test.uacount)]\n",
    "testnu = testnan[~testnan.msno.isin(trainmsno)]\n",
    "for name,group in testnu.groupby(['msno','artist_name']):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name[1]) if sub.strip() in train_art])\n",
    "    indic = len(subcolin) > 0 \n",
    "    overallfreq = uasum[0,2*subcolin].sum()/uasum[0,2*subcolin+1].sum() if indic else 0.5\n",
    "    for ind in group.index:\n",
    "        test.set_value(ind,'uacount',0)\n",
    "        test.set_value(ind,'uafreq',overallfreq)\n",
    "testrem = test[pd.isnull(test.uafreq)]\n",
    "testrem['mind']= testrem.msno.apply(lambda x: userdict[x])\n",
    "for name,group in testrem.groupby('artist_name'):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name) if sub.strip() in train_art])\n",
    "    if len(subcolin) > 0:\n",
    "        vsum = uamat[:,2*subcolin][group.mind,:].sum(axis = 1)\n",
    "        vcount = uamat[:,2*subcolin+1][group.mind,:].sum(axis =1)\n",
    "        gi = group.index\n",
    "        for i in range(group.shape[0]):\n",
    "            test.set_value(gi[i],'uafreq',vsum[i]/vcount[i] if vcount[i] > 0 else 0.5)\n",
    "            test.set_value(gi[i],'uacount',vcount[i]/len(subcolin))\n",
    "    else:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'uacount',0)\n",
    "            test.set_value(ind,'uafreq',group.user_freq[ind])\n",
    "test = test.drop(['mind','amind','uasum'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for name,group in testnu.groupby(['msno','artist_name']):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name[1]) if sub.strip() in train_art])\n",
    "    indic = len(subcolin) > 0 \n",
    "    overallfreq = uasum[0,2*subcolin].sum()/uasum[0,2*subcolin+1].sum() if indic else 0.5\n",
    "    for ind in group.index:\n",
    "        test.set_value(ind,'uacount',0)\n",
    "        test.set_value(ind,'uafreq',overallfreq)\n",
    "testrem = test[pd.isnull(test.uafreq)]\n",
    "testrem['mind']= testrem.msno.apply(lambda x: userdict[x])\n",
    "for name,group in testrem.groupby('artist_name'):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name) if sub.strip() in train_art])\n",
    "    if len(subcolin) > 0:\n",
    "        vsum = uamat[:,2*subcolin][group.mind,:].sum(axis = 1)\n",
    "        vcount = uamat[:,2*subcolin+1][group.mind,:].sum(axis =1)\n",
    "        gi = group.index\n",
    "        for i in range(group.shape[0]):\n",
    "            test.set_value(gi[i],'uafreq',vsum[i]/vcount[i] if vcount[i] > 0 else 0.5)\n",
    "            test.set_value(gi[i],'uacount',vcount[i]/len(subcolin))\n",
    "    else:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'uacount',0)\n",
    "            test.set_value(ind,'uafreq',group.user_freq[ind])\n",
    "test = test.drop(['mind','amind','uasum'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genre Preparation: \n",
    "1. find the intersection of artist between the train and test\n",
    "2. treat the compound artist seperately from single artist\n",
    "3. if a single artist from test set hasn't appeared before, we can do nothing, however, if a compound artist has appeared, we can possibly estimate it from the single artist's contribution\n",
    "4. add a dummy variable genre_estimated to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.genre_ids = train.genre_ids.fillna('-1')\n",
    "test.genre_ids = test.genre_ids.fillna('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_genre = set()\n",
    "for genre in train.genre_ids:\n",
    "    train_genre.add(genre)\n",
    "    if not pd.isnull(genre) and \"|\" in genre:\n",
    "        for sub in genre.split(\"|\"):\n",
    "            train_genre.add(sub)\n",
    "test_genre_diff = set()  #只有五个单个genre没有\n",
    "for name in test.genre_ids.unique():\n",
    "    if not pd.isnull(name) and not \"|\" in name and not name in train_genre:\n",
    "        test_genre_diff.add(name)\n",
    "    if not pd.isnull(name) and \"|\" in name:\n",
    "        indic = [x in train_genre for x in name.split(\"|\")]\n",
    "        if not any(indic):\n",
    "            test_genre_diff.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gsame = set()\n",
    "for genre in test.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and genre in train_genre:\n",
    "        train_gsame.add(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gdump = set()\n",
    "for genre in train.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and not genre in train_gsame:\n",
    "        train_gdump.add(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gsingle = [x for x in train.genre_ids.unique() if not pd.isnull(x) and not \"|\" in x]\n",
    "#test_newsingle = [x for x in test.genre_ids.unique() if not pd.isnull(x) and not \"|\" in x and not x in train_gsingle and x in train_genre]\n",
    "test_newsingle = set()\n",
    "for x in test.genre_ids.unique():\n",
    "    if not pd.isnull(x) and \"|\" in x:\n",
    "        indc = [sub for sub in x.split(\"|\") if sub in train_gsingle]\n",
    "        if len(indc) == 0:\n",
    "            test_newsingle.add(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_compnew = set()\n",
    "for genre in test.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and not genre in train_genre:\n",
    "        for sub in genre.split(\"|\"):\n",
    "            if not sub in train_gsingle and sub in train_genre:\n",
    "                test_compnew.add(genre) \n",
    "#sub= 275/864/857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#prepare user,genre dictionary\n",
    "genredict = dict(zip(list(train_genre),range(len(train_genre))))\n",
    "ugmat = sparse.lil_matrix((len(userdict),2*len(genredict)))\n",
    "v = train.groupby(['msno','genre_ids'])['target'].agg(['sum','count'])\n",
    "v = v.reset_index()\n",
    "v.columns.values[2:] = ['ugsum','ugcount']\n",
    "v['mind'] = v.msno.apply(lambda x: userdict[x])\n",
    "v['gmind'] = v.genre_ids.apply(lambda x: genredict[x])\n",
    "for i in v.index:\n",
    "    ugmat[v.mind[i],2*v.gmind[i]] = v.ugsum[i]\n",
    "    ugmat[v.mind[i],2*v.gmind[i]+1] = v.ugcount[i]\n",
    "ugmat = sparse.csc_matrix(ugmat)\n",
    "for dump in train.genre_ids.unique():\n",
    "    subp = [dumpart for dumpart in dump.split(\"|\") if dumpart not in train_gsingle]\n",
    "    for sub in subp:\n",
    "        ugmat[:,2*genredict[sub]] += ugmat[:,2*genredict[dump]]\n",
    "        ugmat[:,2*genredict[sub]+1] += ugmat[:,2*genredict[dump]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the train features on genre\n",
    "ogenre = train.groupby('genre_ids')['target'].agg(['sum','count'])\n",
    "ogenre = ogenre.reset_index()\n",
    "ogenre.columns.values[1:3] = ['genre_sum','genre_count']\n",
    "train.genre_ids = train.genre_ids.astype('str')\n",
    "ogenre.genre_ids = ogenre.genre_ids.astype('str')\n",
    "train = train.merge(ogenre,on='genre_ids',how = 'left')\n",
    "train['genre_freq'] = (train.genre_sum-train.target+1)/(train.genre_count+1)\n",
    "traindump = train[train.genre_ids.isin(list(train_gdump))]\n",
    "train['is_genre_dump'] = 0\n",
    "ugsum = ugmat.sum(axis = 0)\n",
    "for name,group in traindump.groupby(\"genre_ids\"):\n",
    "    subind = np.array([genredict[sub] for sub in name.split(\"|\")])\n",
    "    gasum = ugsum[0,2*subind].sum()/len(subind)\n",
    "    gacount = ugsum[0,2*subind+1].sum()/len(subind)\n",
    "    for ind in group.index:\n",
    "        train.set_value(ind,'genre_sum',gasum)\n",
    "        train.set_value(ind,'genre_count',gacount)\n",
    "        train.set_value(ind,'genre_freq',gasum/gacount)\n",
    "        train.set_value(ind,'is_genre_dump',1)\n",
    "train = train.drop('genre_sum',1)\n",
    "train.genre_count = train.genre_count -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the test features on genre\n",
    "ogenre['genre_freq'] = ogenre.genre_sum/ogenre.genre_count\n",
    "test = test.merge(ogenre,on='genre_ids',how = 'left')\n",
    "test['is_genre_dump'] = 0\n",
    "testnan = test[pd.isnull(test.genre_sum)]\n",
    "for name, group in testnan.groupby(\"genre_ids\"):\n",
    "    if not \"|\" in name:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'genre_count',0)\n",
    "            test.set_value(ind,'genre_freq',0.5)\n",
    "    else:\n",
    "        subind = np.array([genredict[sub] for sub in name.split(\"|\") if sub in train_genre])\n",
    "        gasum = ugsum[0,2*subind].sum()/len(subind)\n",
    "        gacount = ugsum[0,2*subind+1].sum()/len(subind)\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'genre_count',gacount)\n",
    "            test.set_value(ind,'genre_freq',gasum/gacount)\n",
    "            test.set_value(ind,'is_genre_dump',1)\n",
    "test = test.drop('genre_sum',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the train feature on genre/user pair\n",
    "train = train.merge(v,on = ['msno','genre_ids'],how = 'left')\n",
    "train['ugfreq'] = (train['ugsum'] - train['target']+1)/(train['ugcount']+1)\n",
    "train['ugcount'] = train['ugcount'] -1\n",
    "for name,group in train[train.genre_ids.isin(list(train_gdump))].groupby('genre_ids'):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name.split(\"|\")])\n",
    "    vsum = ugmat[:,2*subcolin][group.mind,:].sum(axis =1)\n",
    "    vcount = ugmat[:,2*subcolin+1][group.mind,:].sum(axis = 1)\n",
    "    gi = group.index ; gs = group.shape[0]\n",
    "    for i in range(gs):\n",
    "        train.set_value(gi[i],'ugfreq',vsum[i]/vcount[i] if vcount[i] > 0 else 0.5)\n",
    "        train.set_value(gi[i],'ugcount',vcount[i]/len(subcolin))\n",
    "train = train.drop(['mind','gmind','ugsum'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#prepare the test feature on genre/user pair\n",
    "trainmsno = train.msno.unique()\n",
    "v['ugfreq'] = v.ugsum/v.ugcount\n",
    "test = test.merge(v,on = ['msno','genre_ids'],how = 'left')\n",
    "testnan = test[pd.isnull(test.ugcount)]\n",
    "testnu = testnan[~testnan.msno.isin(trainmsno)]\n",
    "for name,group in testnu.groupby(['msno','genre_ids']):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name[1].split(\"|\") if sub in train_genre])\n",
    "    overallfreq = ugsum[0,2*subcolin].sum()/ugsum[0,2*subcolin+1].sum()\n",
    "    indic = len(subcolin) > 0\n",
    "    for ind in group.index:\n",
    "        test.set_value(ind,'ugcount',0)\n",
    "        test.set_value(ind,'ugfreq',overallfreq if indic else 0.5)\n",
    "testrem = test[pd.isnull(test.ugfreq)]\n",
    "testrem['mind']= testrem.msno.apply(lambda x: userdict[x])\n",
    "for name,group in testrem.groupby('genre_ids'):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name.split(\"|\") if sub in train_genre])\n",
    "    if len(subcolin) > 0:\n",
    "        vsum = ugmat[:,2*subcolin][group.mind,:].sum(axis = 1)\n",
    "        vcount = ugmat[:,2*subcolin+1][group.mind,:].sum(axis =1)\n",
    "        gi = group.index\n",
    "        for i in range(group.shape[0]):\n",
    "            test.set_value(gi[i],'ugfreq',vsum[i]/vcount[i] if vcount[i] > 0 else 0.5)\n",
    "            test.set_value(gi[i],'ugcount',vcount[i]/len(subcolin))\n",
    "    else:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'ugcount',0)\n",
    "            test.set_value(ind,'ugfreq',group.user_freq[ind])\n",
    "test = test.drop(['mind','gmind','ugsum'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute the cosine similarity based songs score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "rowin = train['msno'].unique()\n",
    "colin = train['song_id'].unique()\n",
    "spm = sparse.lil_matrix((len(rowin),len(colin)))\n",
    "dictuser = dict(zip(rowin,range(len(rowin))))\n",
    "dictsong = dict(zip(colin,range(len(colin))))\n",
    "for ind in train.index:\n",
    "    userp = dictuser[train.msno[ind]]\n",
    "    songp = dictsong[train.song_id[ind]]\n",
    "    spm[userp,songp] = (1 if train.target[ind] == 1 else -1)\n",
    "u_simi_song= cosine_similarity(spm)\n",
    "spm = sparse.csr_matrix(spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['usbs'] = 0.0\n",
    "train['usbs_is_estimate'] = 0\n",
    "train['uind'] = train['msno'].apply(lambda x: dictuser[x])\n",
    "train['rating'] = train['target'].apply(lambda x: 1 if x == 1 else -1)\n",
    "train_gr = train.groupby('song_id')\n",
    "for name,group in train_gr.groupby('song_id'):\n",
    "    num = group.shape[0]\n",
    "    if num > 1:\n",
    "        for ind in group.index:\n",
    "            sims = u_simi_song[group.uind[ind],group.uind]\n",
    "            ra = (np.dot(sims,group.rating)-group.rating[ind])/(num-1)\n",
    "            train.set_value(ind,'usbs',ra)\n",
    "    else:\n",
    "        train.set_value(group.index[0],'usbs',group.uafreq)\n",
    "        train.set_value(group.index[0],'usbs_is_estimate',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test usbs\n",
    "test['usbs'] = 0.0\n",
    "test['usbs_is_estimate'] = 0\n",
    "for ind in test[~test.song_id.isin(train.song_id.unique()) & ~test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',0.5)\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "for ind in test[~test.song_id.isin(train.song_id.unique()) & test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',test.uafreq[ind])\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "for ind in test[test.song_id.isin(train.song_id.unique()) & ~test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',test.song_freq[ind])\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "for name,group in test[test.song_id.isin(train.song_id.unique()) & test.msno.isin(train.msno.unique())].groupby('song_id'):\n",
    "    trainsong = train[train.song_id == name]\n",
    "    for ind in group.index:\n",
    "        sims = u_simi_song[group.uind[ind],trainsong.uind]\n",
    "        ra = np.dot(sims,group.rating)/trainsong.shape[0]\n",
    "        train.set_value(ind,'usbs',ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['uind','rating'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#member clean\n",
    "#How many days has they been active\n",
    "members['rtime'] = members['registration_init_time'].apply(\n",
    "    lambda x : pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "members['etime'] = members['expiration_date'].apply(\n",
    "    lambda x : pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "members['numactivedays'] = (members['etime'] - members['rtime']).dt.days\n",
    "min_day = members['rtime'].min()\n",
    "members['day_id_res'] = (members['rtime'] - min_day).dt.days+1\n",
    "members['day_id_exp'] = (members['etime'] - min_day).dt.days+1\n",
    "members['r_year'] = members['rtime'].dt.year\n",
    "members['r_month'] = members['rtime'].dt.month\n",
    "members['r_day'] = members['rtime'].dt.day\n",
    "members['e_year'] = members['etime'].dt.year\n",
    "members['e_month'] = members['etime'].dt.month\n",
    "members['e_day'] = members['etime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned age and cleaned_gender\n",
    "members['is_male'] = members.gender == 'male'\n",
    "members['is_female'] = members.gender == 'female'\n",
    "members['age_mtrue'] = members.bd.apply(lambda x: 1 if x > 10 and x < 80 else 0)\n",
    "weighted_age = round(members.bd[members.age_mtrue == 1].mean())\n",
    "members['age_clean'] = members.bd.apply(lambda x: x if x >10 and x < 80 else weighted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(members,on = 'msno', how = 'left')\n",
    "test = test.merge(members,on = 'msno', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_extra = pd.read_csv('song_extra_info.csv')\n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n",
    "\n",
    "train = train.merge(songs_extra, on='song_id', how='left')\n",
    "test = test.merge(songs_extra, on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msno', 'city', 'bd', 'gender', 'registered_via',\n",
       "       'registration_init_time', 'expiration_date', 'rtime', 'etime',\n",
       "       'numactivedays', 'day_id_res', 'day_id_exp', 'r_year', 'r_month',\n",
       "       'r_day', 'e_year', 'e_month', 'e_day', 'is_male', 'is_female',\n",
       "       'age_mtrue', 'age_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spm processed by svd\n",
    "#u,s,v = sparse.linalg.svds(spm,k = 120)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
