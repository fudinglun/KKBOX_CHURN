{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import re\n",
    "import xgboost ; import lightgbm as lgb\n",
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "song = pd.read_csv('songs.csv')\n",
    "members =  pd.read_csv('members.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(song,on= 'song_id',how = 'left')\n",
    "test = test.merge(song,on = 'song_id',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_bias, user_freq, song_bias, song_freq\n",
    "tuf = train.groupby('msno')['target'].agg(['sum','count']) \n",
    "tuf = tuf.reset_index()\n",
    "tuf.columns.values[1:3] = ['user_sum','user_count']\n",
    "tuf['user_freq'] = (tuf.user_sum+1)/(tuf.user_count+2) #Laplacian smoothing\n",
    "train = train.merge(tuf,on = 'msno',how = 'left')\n",
    "train.loc[train.user_count == 1,'user_sum'] = 0 \n",
    "train.loc[train.user_count == 1,'user_freq'] = 0.5 \n",
    "train['is_new_user'] = train.user_count == 1\n",
    "\n",
    "#prepare test's user bias\n",
    "test = test.merge(tuf,on = 'msno',how = 'left')\n",
    "test['is_new_user'] = pd.isnull(test.user_count)\n",
    "test.loc[pd.isnull(test.user_count),'user_count'] = 1\n",
    "test.loc[pd.isnull(test.user_sum),'user_sum'] = 0\n",
    "test.loc[pd.isnull(test.user_freq),'user_freq'] = 0.5\n",
    "\n",
    "#prepare songs' bias\n",
    "tuf = train.groupby('song_id')['target'].agg(['sum','count']) #Laplacian smoothing\n",
    "tuf = tuf.reset_index()\n",
    "tuf.columns.values[1:3] = ['song_sum','song_count']\n",
    "tuf['song_freq'] = (tuf.song_sum+1)/(tuf.song_count+2)\n",
    "train = train.merge(tuf,on = 'song_id',how = 'left')\n",
    "train.loc[train.song_count == 1,'song_sum'] = 0\n",
    "train.loc[train.song_count == 1,'song_freq'] = 0.5 \n",
    "train['is_new_song'] = train.song_count == 1\n",
    "\n",
    "#prepare test's songs' bias\n",
    "test = test.merge(tuf,on = 'song_id',how = 'left')\n",
    "test['is_new_song'] = pd.isnull(test.song_count)\n",
    "test.loc[pd.isnull(test.song_count),'song_count'] = 1\n",
    "test.loc[pd.isnull(test.song_freq),'song_freq'] = 0.5\n",
    "test.loc[pd.isnull(test.song_sum),'song_sum'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artist Preparation: \n",
    "1. find the intersection of artist between the train and test\n",
    "2. treat the compound artist seperately from single artist\n",
    "3. if a single artist from test set hasn't appeared before, we can do nothing, however, if a compound artist has appeared, we can possibly estimate it from the single artist's contribution\n",
    "4. add a dummy variable artist_estimated to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    l = re.split(\"[+|&/]\",x)\n",
    "    l = [i.strip() for i in list(l)]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.artist_name = train.artist_name.fillna(\"no_rec\")\n",
    "test.artist_name = test.artist_name.fillna(\"no_rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_art = set()\n",
    "for name in train.artist_name.unique():\n",
    "    train_art.add(name)\n",
    "    for sub in split(name):\n",
    "        train_art.add(sub.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_art_diff = set()\n",
    "for name in test.artist_name.unique():\n",
    "    indic = [sub.strip() in train_art for sub in split(name)]\n",
    "    if not any(indic):\n",
    "        test_art_diff.add(name)\n",
    "#19%的artist从来没有出现过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_csame = set()\n",
    "for name in test.artist_name.unique():\n",
    "     if len(split(name)) > 1 and name in train_art:\n",
    "            tt_csame.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_single = set()\n",
    "for name in train.artist_name.unique():\n",
    "    if len(split(name)) == 1:\n",
    "        tt_single.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_cdump = set()\n",
    "for name in train.artist_name.unique():\n",
    "    if len(split(name)) > 1 and not name in tt_csame:\n",
    "        tt_cdump.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "artdict = dict(zip(list(train_art),range(len(train_art))))\n",
    "userdict = dict(zip(train.msno.unique(),range(train.msno.nunique())))\n",
    "v = train.groupby(['msno','artist_name'])['target'].agg(['sum','count'])\n",
    "v = v.reset_index()\n",
    "v.columns.values[2:] = ['uasum','uacount']\n",
    "uamat = sparse.lil_matrix((len(userdict),2*len(artdict)))\n",
    "v['mind'] = v.msno.apply(lambda x: userdict[x])\n",
    "v['amind'] = v.artist_name.apply(lambda x: artdict[x])\n",
    "for i in v.index:\n",
    "    uamat[v.mind[i],2*v.amind[i]] = v.uasum[i]\n",
    "    uamat[v.mind[i],2*v.amind[i]+1] = v.uacount[i]\n",
    "disdict = {}\n",
    "for key, ind in artdict.items():\n",
    "    l = [artdict[sub] for sub in split(key) if not sub in tt_single]\n",
    "    if len(l) > 0:\n",
    "        disdict[key] = (l,ind)\n",
    "uamat = sparse.csc_matrix(uamat)\n",
    "for key, ind in disdict.items():\n",
    "    for sub in ind[0]:\n",
    "        uamat[:,2*sub] += uamat[:,2*ind[1]]\n",
    "        uamat[:,2*sub+1] += uamat[:,2*ind[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  212,   319,   335, ..., 27897, 27966, 30423], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uamat.data = uamat.data.astype('int64')\n",
    "#np.savez(\"uamat.csv\",data = uamat.data,indices=uamat.indices,indptr =uamat.indptr, shape=uamat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train artist feature\n",
    "oart = train.groupby('artist_name')['target'].agg(['sum','count'])\n",
    "oart = oart.reset_index()\n",
    "oart.columns.values[1:3] = ['art_sum','art_count']\n",
    "oart['art_freq'] = (oart.art_sum+1)/(oart.art_count+2)\n",
    "train = train.merge(oart,on='artist_name',how = 'left')\n",
    "train.loc[train.art_count == 1,'art_sum'] = 0\n",
    "train.loc[train.art_count == 1,'art_freq'] = 0.5\n",
    "train['is_new_art'] = train.art_count == 1\n",
    "traindump = train[train.artist_name.isin(list(tt_cdump))]\n",
    "train['is_art_dump'] = 0\n",
    "uasum = uamat.sum(axis = 0)\n",
    "for name,group in traindump.groupby(\"artist_name\"):\n",
    "    subind = np.array([artdict[sub.strip()] for sub in split(name)])\n",
    "    gasum = uasum[0,2*subind].sum()/len(subind)\n",
    "    gacount = uasum[0,2*subind+1].sum()/len(subind)\n",
    "    for ind in group.index:\n",
    "        train.set_value(ind,'art_sum',gasum)\n",
    "        train.set_value(ind,'art_count',gacount)\n",
    "        train.set_value(ind,'art_freq',(gasum+1)/(gacount+2))\n",
    "        train.set_value(ind,'is_art_dump',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare test artist feature\n",
    "test = test.merge(oart,on='artist_name',how = 'left')\n",
    "test['is_art_dump'] = 0\n",
    "test['is_new_art'] = 0\n",
    "testnan = test[pd.isnull(test.art_sum)]\n",
    "test.loc[pd.isnull(test.art_sum),'is_new_art'] = 1\n",
    "for name, group in testnan.groupby(\"artist_name\"):\n",
    "    subind = np.array([artdict[sub.strip()] for sub in split(name) if sub.strip() in train_art])\n",
    "    if subind.size == 0:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'art_sum',0)\n",
    "            test.set_value(ind,'art_count',1)\n",
    "            test.set_value(ind,'art_freq',0.5)\n",
    "    else:\n",
    "        gasum = uasum[0,2*subind].sum()/len(subind)\n",
    "        gacount = uasum[0,2*subind+1].sum()/len(subind)\n",
    "        for ind in group.index:\n",
    "            train.set_value(ind,'art_sum',gasum)\n",
    "            test.set_value(ind,'art_count',gacount)\n",
    "            test.set_value(ind,'art_freq',(gasum+1)/(gacount+2))\n",
    "            test.set_value(ind,'is_art_dump',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train/artist feature\n",
    "v['uafreq'] = (v.uasum+1)/(v.uacount+2)\n",
    "train = train.merge(v,on = ['msno','artist_name'],how = 'left')\n",
    "train.loc[train.uacount == 1,'uafreq'] = 0.5\n",
    "train.loc[train.uacount == 1,'uasum'] = 0\n",
    "for name,group in train[train.artist_name.isin(list(tt_cdump))].groupby('artist_name'):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name)])\n",
    "    vsum = uamat[:,2*subcolin][group.mind,:].sum(axis =1)\n",
    "    vcount = uamat[:,2*subcolin+1][group.mind,:].sum(axis = 1)\n",
    "    gi = group.index ; gs = group.shape[0]\n",
    "    for i in range(gs):\n",
    "        train.set_value(gi[i],'uasum',vsum[i]/len(subcolin))\n",
    "        train.set_value(gi[i],'uacount',vcount[i]/len(subcolin))\n",
    "        train.set_value(gi[i],'uafreq',(vsum[i]+len(subcolin))/(vcount[i]+2*len(subcolin)) \n",
    "                        if vcount[i] > len(subcolin) else 0.5)\n",
    "train = train.drop(['mind','amind'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#prepare test/artist feature\n",
    "trainmsno = train.msno.unique()\n",
    "test = test.merge(v,on = ['msno','artist_name'],how = 'left')\n",
    "testnan = test[pd.isnull(test.uacount)]\n",
    "testnu = testnan[~testnan.msno.isin(trainmsno)]\n",
    "for name,group in testnu.groupby(['msno','artist_name']):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name[1]) if sub.strip() in train_art])\n",
    "    indic = len(subcolin) > 0 \n",
    "    gasum = uasum[0,2*subcolin].sum()/len(subcolin) if indic else 1\n",
    "    gacount = uasum[0,2*subcolin+1].sum()/len(subcolin) if indic else 1\n",
    "    overallfreq = (gasum + len(subcolin))/(gacount + 2*len(subcolin)) if indic else 0.5\n",
    "    for ind in group.index:\n",
    "        test.set_value(ind,'uasum', gasum if indic else 0)\n",
    "        test.set_value(ind,'uacount',gacount if indic else 1)\n",
    "        test.set_value(ind,'uafreq',overallfreq)\n",
    "testrem = test[pd.isnull(test.uafreq)]\n",
    "testrem['mind']= testrem.msno.apply(lambda x: userdict[x])\n",
    "for name,group in testrem.groupby('artist_name'):\n",
    "    subcolin = np.array([artdict[sub.strip()] for sub in split(name) if sub.strip() in train_art])\n",
    "    if len(subcolin) > 0:\n",
    "        vsum = uamat[:,2*subcolin][group.mind,:].sum(axis = 1)\n",
    "        vcount = uamat[:,2*subcolin+1][group.mind,:].sum(axis =1)\n",
    "        gi = group.index\n",
    "        for i in range(group.shape[0]):\n",
    "            test.set_value(gi[i],'uafreq',(vsum[i]+len(subcolin))/(vcount[i] + 2*len(subcolin))\n",
    "                           if vcount[i] > len(subcolin) else 0.5)\n",
    "            test.set_value(gi[i],'uacount',vcount[i]/len(subcolin))\n",
    "            test.set_value(gi[i],'uasum',vsum[i]/len(subcolin))\n",
    "    else:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'uasum',0)\n",
    "            test.set_value(ind,'uacount',1)\n",
    "            test.set_value(ind,'uafreq',0.5)\n",
    "test = test.drop(['mind','amind'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genre Preparation: \n",
    "1. find the intersection of artist between the train and test\n",
    "2. treat the compound artist seperately from single artist\n",
    "3. if a single artist from test set hasn't appeared before, we can do nothing, however, if a compound artist has appeared, we can possibly estimate it from the single artist's contribution\n",
    "4. add a dummy variable genre_estimated to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.genre_ids = train.genre_ids.fillna('-1')\n",
    "test.genre_ids = test.genre_ids.fillna('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_genre = set()\n",
    "for genre in train.genre_ids:\n",
    "    train_genre.add(genre)\n",
    "    if not pd.isnull(genre) and \"|\" in genre:\n",
    "        for sub in genre.split(\"|\"):\n",
    "            train_genre.add(sub)\n",
    "test_genre_diff = set()  #只有五个单个genre没有\n",
    "for name in test.genre_ids.unique():\n",
    "    if not pd.isnull(name) and not \"|\" in name and not name in train_genre:\n",
    "        test_genre_diff.add(name)\n",
    "    if not pd.isnull(name) and \"|\" in name:\n",
    "        indic = [x in train_genre for x in name.split(\"|\")]\n",
    "        if not any(indic):\n",
    "            test_genre_diff.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gsame = set()\n",
    "for genre in test.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and genre in train_genre:\n",
    "        train_gsame.add(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gdump = set()\n",
    "for genre in train.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and not genre in train_gsame:\n",
    "        train_gdump.add(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gsingle = [x for x in train.genre_ids.unique() if not pd.isnull(x) and not \"|\" in x]\n",
    "#test_newsingle = [x for x in test.genre_ids.unique() if not pd.isnull(x) and not \"|\" in x and not x in train_gsingle and x in train_genre]\n",
    "test_newsingle = set()\n",
    "for x in test.genre_ids.unique():\n",
    "    if not pd.isnull(x) and \"|\" in x:\n",
    "        indc = [sub for sub in x.split(\"|\") if sub in train_gsingle]\n",
    "        if len(indc) == 0:\n",
    "            test_newsingle.add(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_compnew = set()\n",
    "for genre in test.genre_ids.unique():\n",
    "    if not pd.isnull(genre) and \"|\" in genre and not genre in train_genre:\n",
    "        for sub in genre.split(\"|\"):\n",
    "            if not sub in train_gsingle and sub in train_genre:\n",
    "                test_compnew.add(genre) \n",
    "#sub= 275/864/857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#prepare user,genre dictionary\n",
    "genredict = dict(zip(list(train_genre),range(len(train_genre))))\n",
    "ugmat = sparse.lil_matrix((len(userdict),2*len(genredict)))\n",
    "v = train.groupby(['msno','genre_ids'])['target'].agg(['sum','count'])\n",
    "v = v.reset_index()\n",
    "v.columns.values[2:] = ['ugsum','ugcount']\n",
    "v['mind'] = v.msno.apply(lambda x: userdict[x])\n",
    "v['gmind'] = v.genre_ids.apply(lambda x: genredict[x])\n",
    "for i in v.index:\n",
    "    ugmat[v.mind[i],2*v.gmind[i]] = v.ugsum[i]\n",
    "    ugmat[v.mind[i],2*v.gmind[i]+1] = v.ugcount[i]\n",
    "ugmat = sparse.csc_matrix(ugmat)\n",
    "for dump in train.genre_ids.unique():\n",
    "    subp = [dumpart for dumpart in dump.split(\"|\") if dumpart not in train_gsingle]\n",
    "    for sub in subp:\n",
    "        ugmat[:,2*genredict[sub]] += ugmat[:,2*genredict[dump]]\n",
    "        ugmat[:,2*genredict[sub]+1] += ugmat[:,2*genredict[dump]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the train features on genre\n",
    "ogenre = train.groupby('genre_ids')['target'].agg(['sum','count'])\n",
    "ogenre = ogenre.reset_index()\n",
    "ogenre.columns.values[1:3] = ['genre_sum','genre_count']\n",
    "ogenre['genre_freq'] = (ogenre.genre_sum+1)/(ogenre.genre_count+2)\n",
    "train.genre_ids = train.genre_ids.astype('str')\n",
    "ogenre.genre_ids = ogenre.genre_ids.astype('str')\n",
    "train = train.merge(ogenre,on='genre_ids',how = 'left')\n",
    "train['is_new_genre'] = train.genre_count == 1\n",
    "train.loc[train.genre_count ==1, 'genre_sum'] = 0\n",
    "train.loc[train.genre_count ==1, 'genre_freq'] = 0.5\n",
    "traindump = train[train.genre_ids.isin(list(train_gdump))]\n",
    "train['is_genre_dump'] = 0\n",
    "ugsum = ugmat.sum(axis = 0)\n",
    "for name,group in traindump.groupby(\"genre_ids\"):\n",
    "    subind = np.array([genredict[sub] for sub in name.split(\"|\")])\n",
    "    gasum = ugsum[0,2*subind].sum()/len(subind)\n",
    "    gacount = ugsum[0,2*subind+1].sum()/len(subind)\n",
    "    for ind in group.index:\n",
    "        train.set_value(ind,'genre_sum',gasum)\n",
    "        train.set_value(ind,'genre_count',gacount)\n",
    "        train.set_value(ind,'genre_freq',(gasum+1)/(gacount+2))\n",
    "        train.set_value(ind,'is_genre_dump',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the test features on genre\n",
    "test = test.merge(ogenre,on='genre_ids',how = 'left')\n",
    "test['is_new_genre'] = 0\n",
    "test['is_genre_dump'] = 0\n",
    "test.loc[pd.isnull(test.genre_sum),'is_new_genre'] = 1\n",
    "testnan = test[pd.isnull(test.genre_sum)]\n",
    "for name, group in testnan.groupby(\"genre_ids\"):\n",
    "    if not \"|\" in name:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'genre_count',0)\n",
    "            test.set_value(ind,'genre_freq',0.5)\n",
    "    else:\n",
    "        subind = np.array([genredict[sub] for sub in name.split(\"|\") if sub in train_genre])\n",
    "        gasum = ugsum[0,2*subind].sum()/len(subind)\n",
    "        gacount = ugsum[0,2*subind+1].sum()/len(subind)\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'genre_sum',gasum)\n",
    "            test.set_value(ind,'genre_count',gacount)\n",
    "            test.set_value(ind,'genre_freq',(gasum+1)/(gacount+2))\n",
    "            test.set_value(ind,'is_genre_dump',1)\n",
    "#test = test.drop('genre_sum',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the train feature on genre/user pair\n",
    "v['ugfreq'] = (v.ugsum+1)/(v.ugcount+2)\n",
    "train = train.merge(v,on = ['msno','genre_ids'],how = 'left')\n",
    "train.loc[train.uacount == 1,'uafreq'] = 0.5\n",
    "train.loc[train.uacount == 1,'uasum'] = 0\n",
    "for name,group in train[train.genre_ids.isin(list(train_gdump))].groupby('genre_ids'):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name.split(\"|\")])\n",
    "    vsum = ugmat[:,2*subcolin][group.mind,:].sum(axis =1)\n",
    "    vcount = ugmat[:,2*subcolin+1][group.mind,:].sum(axis = 1)\n",
    "    gi = group.index ; gs = group.shape[0]\n",
    "    for i in range(gs):\n",
    "        train.set_value(gi[i],'ugfreq',(vsum[i]+len(subcolin))/(vcount[i] + 2*len(subcolin))\n",
    "                        if vcount[i] > len(subcolin) else 0.5)\n",
    "        train.set_value(gi[i],'ugcount',vcount[i]/len(subcolin))\n",
    "        train.set_value(gi[i],'ugsum',vsum[i]/len(subcolin))\n",
    "train = train.drop(['mind','gmind'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#prepare the test feature on genre/user pair\n",
    "trainmsno = train.msno.unique()\n",
    "test = test.merge(v,on = ['msno','genre_ids'],how = 'left')\n",
    "testnan = test[pd.isnull(test.ugcount)]\n",
    "testnu = testnan[~testnan.msno.isin(trainmsno)]\n",
    "for name,group in testnu.groupby(['msno','genre_ids']):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name[1].split(\"|\") if sub in train_genre])\n",
    "    indic = len(subcolin) > 0\n",
    "    gasum = ugsum[0,2*subcolin].sum()/len(subcolin) if indic else 1\n",
    "    gacount = ugsum[0,2*subcolin+1].sum()/len(subcolin) if indic else 1\n",
    "    overallfreq = (gasum + len(subcolin))/(gacount + 2*len(subcolin)) if indic else 0.5\n",
    "    for ind in group.index:\n",
    "        test.set_value(ind,'ugcount',gacount if indic else 1)\n",
    "        test.set_value(ind,'ugsum',gasum if indic else 0)\n",
    "        test.set_value(ind,'ugfreq',overallfreq)\n",
    "testrem = test[pd.isnull(test.ugfreq)]\n",
    "testrem['mind']= testrem.msno.apply(lambda x: userdict[x])\n",
    "for name,group in testrem.groupby('genre_ids'):\n",
    "    subcolin = np.array([genredict[sub.strip()] for sub in name.split(\"|\") if sub in train_genre])\n",
    "    if len(subcolin) > 0:\n",
    "        vsum = ugmat[:,2*subcolin][group.mind,:].sum(axis = 1)\n",
    "        vcount = ugmat[:,2*subcolin+1][group.mind,:].sum(axis =1)\n",
    "        gi = group.index\n",
    "        for i in range(group.shape[0]):\n",
    "            test.set_value(gi[i],'ugfreq',(vsum[i]+len(subcolin))/(vcount[i] + 2*len(subcolin))\n",
    "                           if vcount[i] > len(subcolin) else 0.5)\n",
    "            test.set_value(gi[i],'ugcount',vcount[i]/len(subcolin))\n",
    "            test.set_value(gi[i],'ugsum',vsum[i]/len(subcolin))\n",
    "    else:\n",
    "        for ind in group.index:\n",
    "            test.set_value(ind,'ugsum',0)\n",
    "            test.set_value(ind,'ugcount',1)\n",
    "            test.set_value(ind,'ugfreq',0.5)\n",
    "test = test.drop(['mind','gmind'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute the cosine similarity based songs score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "rowin = train['msno'].unique()\n",
    "colin = train['song_id'].unique()\n",
    "spm = sparse.lil_matrix((len(rowin),len(colin)))\n",
    "dictuser = dict(zip(rowin,range(len(rowin))))\n",
    "dictsong = dict(zip(colin,range(len(colin))))\n",
    "for ind in train.index:\n",
    "    userp = dictuser[train.msno[ind]]\n",
    "    songp = dictsong[train.song_id[ind]]\n",
    "    spm[userp,songp] = (1 if train.target[ind] == 1 else -1)\n",
    "u_simi_song= cosine_similarity(spm)\n",
    "#spm = sparse.csr_matrix(spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['usbs'] = 0.0\n",
    "train['usbs_is_estimate'] = 0\n",
    "train['uind'] = train['msno'].apply(lambda x: dictuser[x])\n",
    "train['rating'] = train['target'].apply(lambda x: 1 if x == 1 else -1)\n",
    "train_gr = train.groupby('song_id')\n",
    "for name,group in train_gr:\n",
    "    num = group.shape[0]\n",
    "    if num > 1:\n",
    "        for ind in group.index:\n",
    "            sims = u_simi_song[group.uind[ind],group.uind]\n",
    "            ra = (np.dot(sims,group.rating)-group.rating[ind])/(num-1)\n",
    "            train.set_value(ind,'usbs',ra)\n",
    "#     else:\n",
    "#         train.set_value(group.index[0],'usbs',group.uafreq)\n",
    "#         train.set_value(group.index[0],'usbs_is_estimate',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test usbs\n",
    "test['usbs'] = 0.0\n",
    "test['usbs_is_estimate'] = 0\n",
    "for ind in test[~test.song_id.isin(train.song_id.unique()) & ~test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',0.5)\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "for ind in test[~test.song_id.isin(train.song_id.unique()) & test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',test.uafreq[ind])\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "for ind in test[test.song_id.isin(train.song_id.unique()) & ~test.msno.isin(train.msno.unique())].index:\n",
    "    test.set_value(ind,'usbs',test.song_freq[ind])\n",
    "    test.set_value(ind,'usbs_is_estimate',1)\n",
    "testall = test[test.song_id.isin(train.song_id.unique()) & test.msno.isin(train.msno.unique())]\n",
    "testall['uind'] = testall.msno.apply(lambda x: dictuser[x])\n",
    "for name,group in testall.groupby('song_id'):\n",
    "    trainind = train.loc[train.song_id == name,'uind']\n",
    "    for ind in group.index:\n",
    "        sims = u_simi_song[group.uind[ind],trainind]\n",
    "        ra = np.dot(sims,group.rating)/trainind.size\n",
    "        test.set_value(ind,'usbs',ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d89cdb815bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtestall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uind'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdictuser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'song_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_simi_song\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp11/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[0;32m~/anaconda3/envs/nlp11/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp11/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testall = test[test.song_id.isin(train.song_id.unique()) & test.msno.isin(train.msno.unique())]\n",
    "testall['uind'] = testall.msno.apply(lambda x: dictuser[x])\n",
    "for name,group in testall.groupby('song_id'):\n",
    "    trainind = train.loc[train.song_id == name,['uind','rating']]\n",
    "    for ind in group.index:\n",
    "        sims = u_simi_song[group.uind[ind],trainind.uind]\n",
    "        ra = np.dot(sims,trainind.rating)/trainind.shape[0]\n",
    "        test.set_value(ind,'usbs',ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556790, 13)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in testall.msno.unique():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#member clean\n",
    "#How many days has they been active\n",
    "members['rtime'] = members['registration_init_time'].apply(\n",
    "    lambda x : pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "members['etime'] = members['expiration_date'].apply(\n",
    "    lambda x : pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "members['numactivedays'] = (members['etime'] - members['rtime']).dt.days\n",
    "min_day = members['rtime'].min()\n",
    "members['day_id_res'] = (members['rtime'] - min_day).dt.days+1\n",
    "members['day_id_exp'] = (members['etime'] - min_day).dt.days+1\n",
    "members['r_year'] = members['rtime'].dt.year\n",
    "members['r_month'] = members['rtime'].dt.month\n",
    "members['r_day'] = members['rtime'].dt.day\n",
    "members['e_year'] = members['etime'].dt.year\n",
    "members['e_month'] = members['etime'].dt.month\n",
    "members['e_day'] = members['etime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned age and cleaned_gender\n",
    "members['is_male'] = members.gender == 'male'\n",
    "members['is_female'] = members.gender == 'female'\n",
    "members['age_mtrue'] = members.bd.apply(lambda x: 1 if x > 10 and x < 80 else 0)\n",
    "weighted_age = round(members.bd[members.age_mtrue == 1].mean())\n",
    "members['age_clean'] = members.bd.apply(lambda x: x if x >10 and x < 80 else weighted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(members,on = 'msno', how = 'left')\n",
    "test = test.merge(members,on = 'msno', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs_extra = pd.read_csv('song_extra_info.csv')\n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n",
    "\n",
    "train = train.merge(songs_extra, on='song_id', how='left')\n",
    "test = test.merge(songs_extra, on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['uind','rating'],1)\n",
    "ttrain = train.drop(['usbs','usbs_is_estimate'],1)\n",
    "ttest = test.drop('id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttrain = train\n",
    "ttest = test.drop('id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_keep = ['user_count', 'user_freq', 'song_count','song_freq', 'art_count', 'art_freq', 'is_art_dump', 'uacount','uafreq', \n",
    "'genre_count', 'genre_freq', 'is_genre_dump', 'ugcount','ugfreq','song_length','is_male','is_female','age_mtrue',\n",
    "'age_clean','song_year','numactivedays','source_system_tab','source_type','registered_via','source_screen_name','city',\n",
    "'day_id_res','day_id_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttest = test[columns_keep]\n",
    "ttrain = train[columns_keep+['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ttest['song_length'] = round(ttest['song_length']/60000,2)\n",
    "ttrain['song_length'] = round(ttrain['song_length']/60000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataexpand(column,ttrain,ttest):\n",
    "    stored = pd.get_dummies(pd.concat([ttrain[column],ttest[column]]))\n",
    "    stored.columns = [column+'_'+str(i) for i in stored.columns.values]\n",
    "    ttrain = pd.concat([ttrain,stored.iloc[:ttrain.shape[0],:]],axis = 1)\n",
    "    ttest = pd.concat([ttest,stored.iloc[ttrain.shape[0]:,:]],axis = 1)\n",
    "    ttrain = ttrain.drop(column,1)\n",
    "    ttest = ttest.drop(column,1)\n",
    "    return ttrain,ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_system_tab\n",
      "source_type\n",
      "registered_via\n",
      "source_screen_name\n",
      "city\n"
     ]
    }
   ],
   "source": [
    "for column in ['source_system_tab','source_type','registered_via','source_screen_name','city']:\n",
    "    print(column)\n",
    "    ttrain,ttest = dataexpand(column,ttrain,ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ttrain.columns:\n",
    "    if any(pd.isnull(ttrain[column])):\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.07982894421\n",
      "2011.2265781\n"
     ]
    }
   ],
   "source": [
    "for column in ['song_length','song_year']:\n",
    "    ttrain['nan_'+column] = pd.isnull(ttrain[column])\n",
    "    ttest['nan_'+column] = pd.isnull(ttest[column])\n",
    "    ml = pd.concat([ttrain[column],ttest[column]]).mean()\n",
    "    print(ml)\n",
    "    ttrain.loc[:,column] = ttrain.loc[:,column].fillna(ml)\n",
    "    ttest.loc[:,column] = ttest.loc[:,column].fillna(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_count\n",
      "song_count\n",
      "art_count\n",
      "uacount\n",
      "genre_count\n",
      "ugcount\n",
      "song_length\n",
      "age_clean\n",
      "song_year\n",
      "numactivedays\n",
      "day_id_res\n",
      "day_id_exp\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "for column in ['user_count','song_count','art_count','uacount','genre_count','ugcount','song_length','age_clean','song_year',\n",
    "              'numactivedays','day_id_res','day_id_exp']:\n",
    "    print(column)\n",
    "    nmax = max(ttrain[column].max(),ttest[column].max())\n",
    "    nmin = min(ttrain[column].min(),ttest[column].min())\n",
    "    diff = nmax - nmin\n",
    "    ttrain[column] = (ttrain[column] - nmin)/diff\n",
    "    ttest[column] = (ttest[column] - nmin)/diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranind = np.random.permutation(range(ttrain.shape[0]))\n",
    "thres = round(ttrain.shape[0]*0.75)\n",
    "m = ttrain.drop('target',1).iloc[ranind[:thres],:]\n",
    "n = ttrain.drop('target',1).iloc[ranind[thres:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(pd.isnull(ttrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'roc_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-1bdc380dfd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_curve' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for pe in [0.05,0.1,0.5,1]:\n",
    "    print(pe)\n",
    "    model = LogisticRegression(C = pe,solver = 'sag')\n",
    "    model.fit(m,train.target[ranind[:thres]])\n",
    "    v = model.predict_proba(n)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(train.target[ranind[thres:]],v)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869685176733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "for pe in [1]:\n",
    "    print(pe)\n",
    "    model = LogisticRegression(penalty = 'l1',C = pe,solver = 'saga')\n",
    "    model.fit(m,train.target[ranind[:thres]])\n",
    "    v = model.predict_proba(n)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(train.target[ranind[thres:]],v)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = model.predict_proba(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='saga', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ttrain.drop('target',1),ttrain.target)\n",
    "c2 = model.predict(ttest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.71015704503128485"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c2 - l).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['user_freq', 'song_count', 'song_freq', 'uafreq', 'ugfreq',\n",
       "       'source_system_tab_my library', 'source_system_tab_search',\n",
       "       'source_type_local-playlist',\n",
       "       'source_screen_name_Local playlist more'], dtype=object)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest.columns.values[(model.coef_ > 0.01)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_column = ['user_count', 'user_freq', 'song_count', 'song_freq', 'uacount','uafreq', 'ugcount', 'ugfreq', 'is_female', 'age_clean',\n",
    "'song_year', 'day_id_res', 'day_id_exp', 'source_type_artist','source_type_local-library', 'source_type_local-playlist',\n",
    "'source_type_topic-article-playlist', 'registered_via_7','registered_via_13', 'source_screen_name_Discover New','source_system_tab_my library', \n",
    "'source_system_tab_search','source_screen_name_My library_Search', 'source_screen_name_Search','nan_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-6c2f77ddc340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrbf_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'polynomial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "rbf_svc = svm.SVC(kernel='polynomial',C = 0.5)\n",
    "model.fit(m,train.target[ranind[:thres]])\n",
    "v = model.predict_proba(n)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(train.target[ranind[thres:]],v)\n",
    "roc_auc = auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropcol = ['user_count', 'user_freq', 'song_count', 'song_freq', 'art_count',\n",
    "       'art_freq', 'is_art_dump', 'uacount', 'uafreq', 'genre_count',\n",
    "       'genre_freq', 'is_genre_dump', 'ugcount', 'ugfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = model.predict_proba(ttest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50000561,  0.50001856,  0.49999564,  0.49999965,  0.49999438,\n",
       "        0.49999967,  0.5       ,  0.5000073 ,  0.50000121,  0.50001479,\n",
       "        0.50002172,  0.49999818,  0.50000394,  0.49999356,  0.49999914,\n",
       "        0.49999667,  0.49999415,  0.4999934 ,  0.49999966,  0.49999728,\n",
       "        0.5000314 ,  0.5000055 ,  0.50001058,  0.50000741,  0.500025  ,\n",
       "        0.50002613,  0.50002501,  0.50002463,  0.50002502,  0.49999744,\n",
       "        0.50002591,  0.5000273 ,  0.50002568,  0.50002513,  0.50000334,\n",
       "        0.49998411,  0.50002093,  0.49998393,  0.50002068,  0.50001901,\n",
       "        0.50000894,  0.5000036 ,  0.50000896,  0.50001199,  0.50000892,\n",
       "        0.50001401,  0.49997602,  0.500006  ,  0.50000642,  0.50000606,\n",
       "        0.50000149,  0.50001165,  0.50001373,  0.50000753,  0.50001481,\n",
       "        0.50000877,  0.5000032 ,  0.49999691,  0.49999309,  0.50002371,\n",
       "        0.50000816,  0.50001859,  0.49999894,  0.50000013,  0.49998628,\n",
       "        0.50000797,  0.49999656,  0.50000695,  0.50000855,  0.50000532,\n",
       "        0.50000122,  0.50000048,  0.50000636,  0.5000074 ,  0.50000399,\n",
       "        0.50000501,  0.50001622,  0.5000162 ,  0.5000162 ,  0.50001623,\n",
       "        0.50000945,  0.50001101,  0.50000962,  0.49998477,  0.49999421,\n",
       "        0.49998379,  0.4999867 ,  0.50000793,  0.50000752,  0.4999928 ,\n",
       "        0.50001179,  0.50000457,  0.50000979,  0.49999917,  0.49999261,\n",
       "        0.50002171,  0.49999184,  0.49999712,  0.5000056 ,  0.50000656])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79007948025688468"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (C = pe,solver = 'sag')\n",
    "model.fit(m[dropcol],train.target[ranind[:thres]])\n",
    "v = model.predict_proba(n[dropcol])[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(train.target[ranind[thres:]],v)\n",
    "roc_auc = auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(criterion = 'entropy',max_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(m,train.target[ranind[:thres]])\n",
    "v = clf.predict_proba(n)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(train.target[ranind[thres:]],v)\n",
    "roc_auc = auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = clf.predict_proba(ttest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86096054951812229"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x39db7c358>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(train.art_freq,train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "gbes = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "                                           validation_fraction=0.2,\n",
    "                                           n_iter_no_change=5, tol=0.01,\n",
    "                                           random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.DataFrame()\n",
    "subm['id'] = test.id\n",
    "subm['target'] = p_test_1\n",
    "subm.to_csv(\"hope.csv.gz\",compression = 'gzip',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.DataFrame()\n",
    "subm['id'] = test.id\n",
    "subm['target'] = 1-l\n",
    "subm.to_csv(\"hope.csv.gz\",compression = 'gzip',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song_length\n",
      "song_year\n"
     ]
    }
   ],
   "source": [
    "for column in ttrain.columns:\n",
    "    if any(pd.isnull(ttrain[column])):\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttrain['nan_length'] = pd.isnull(train.song_length)\n",
    "ttrain['nan_year'] = pd.isnull(train.song_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test and validation sets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-6f773e45101b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# del train, test; gc.collect();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0md_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mwatchlist_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processed data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Train test and validation sets\")\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "\n",
    "X_train = train.drop(['target'], axis=1)\n",
    "y_train = train['target'].values\n",
    "\n",
    "\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "ids = test['id'].values\n",
    "\n",
    "X_train = X_train.drop(['rtime','etime'],1)\n",
    "X_test = X_test.drop(['rtime','etime'],1)\n",
    "\n",
    "# del train, test; gc.collect();\n",
    "\n",
    "d_train_final = lgb.Dataset(X_train, y_train)\n",
    "watchlist_final = lgb.Dataset(X_train, y_train)\n",
    "print('Processed data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train_final = lgb.Dataset(X_train, y_train)\n",
    "watchlist_final = lgb.Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-3ea2e6e80bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong_year\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp11/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "X_train.song_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalid_0's auc: 0.884988\n",
      "[10]\tvalid_0's auc: 0.889976\n",
      "[15]\tvalid_0's auc: 0.892889\n",
      "[20]\tvalid_0's auc: 0.895368\n",
      "CPU times: user 1min 41s, sys: 876 ms, total: 1min 42s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.3 ,\n",
    "        'verbose': 0,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'max_depth': 8,\n",
    "        'num_rounds': 20,\n",
    "        'metric' : 'auc'\n",
    "    }\n",
    "\n",
    "%time model_f1 = lgb.train(params, train_set=d_train_final,  valid_sets=watchlist_final, verbose_eval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 54,  2, 24, 23,  0,  5, 12,  3,  3,  0,  1,  6,  8,  0, 33, 15,\n",
       "       60,  0, 15,  2, 15,  0,  0, 51, 45, 73,  1,  0,  1,  0,  0, 27, 23,\n",
       "       40,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f1.feature_importance(importance_type='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions')\n",
    "p_test_1 = model_f1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34792644,  0.45328587,  0.10034968,  0.10826129,  0.04545351,\n",
       "        0.13946447,  0.30898531,  0.87795143,  0.2658625 ,  0.13572496])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test_1[:10]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
